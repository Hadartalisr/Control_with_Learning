\documentclass[a4 paper]{article}
% Set target color model to RGB
\usepackage[inner=2.0cm,outer=2.0cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{setspace}
\usepackage[rgb]{xcolor}
\usepackage{verbatim}
\usepackage{subcaption}
\usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,tikz,amssymb}
\usepackage{fancyhdr}
\usepackage[colorlinks=true, urlcolor=blue,  linkcolor=blue, citecolor=blue]{hyperref}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing} % Add this line to load the necessary TikZ library for decorations


%\usetikzlibrary{through,backgrounds}
\hypersetup{%
pdfauthor={Ashudeep Singh},%
pdftitle={Homework},%
pdfkeywords={Tikz,latex,bootstrap,uncertaintes},%
pdfcreator={PDFLaTeX},%
pdfproducer={PDFLaTeX},%
}
%\usetikzlibrary{shadows}
% \usepackage[francais]{babel}
\usepackage{booktabs}
\input{macros.tex}



\begin{document}
\homework{Assignment 1 - LQR Control}{Due: 02/06/24}{Oron Sabag}{}{Hadar Tal}{}
\textbf{Instructions}: 
\begin{itemize}
    \item The assignment is to be done individually.
    \item Submit your assignment as a single PDF file.
    \item Read all the Questions carefully before you start working on the assignment.
    \item This file contains \underbar{extra materials} for those who are interested in learning the tools used in the 
        assignment's solution. \underbar{Reading it is not required to complete the assignment.}
\end{itemize}

% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\section{Continuous-time System}

In the lecture, we focused on discrete linear systems. However, real-world systems often operate in continuous time, 
making it essential to understand the transition to continuous-time systems and the associated control strategies.

\subsection{Motivation for Continuous-time Systems}
Continuous-time systems are ubiquitous in engineering and natural processes. Examples include electrical circuits, 
mechanical systems, and biological systems. Unlike discrete systems, which are defined at specific time intervals, 
continuous systems evolve over time according to differential equations. This continuous evolution provides a more 
accurate representation of physical phenomena, allowing for precise modeling and control.

\subsection{Equations of the Dynamics}
The state-space representation of a continuous-time linear system is given by the following set of differential equations:

\begin{equation}
    \dot{x}(t) = A x(t) + B u(t),
\end{equation}
\begin{equation}
    y(t) = C x(t) + D u(t),
\end{equation}

where:
\begin{itemize}
    \item $x(t) \in \mathbb{R}^n$ is the state vector.
    \item $u(t) \in \mathbb{R}^m$ is the control input.
    \item $y(t) \in \mathbb{R}^p$ is the output vector.
    \item $A \in \mathbb{R}^{n \times n}$ is the state matrix.
    \item $B \in \mathbb{R}^{n \times m}$ is the input matrix.
    \item $C \in \mathbb{R}^{p \times n}$ is the output matrix.
    \item $D \in \mathbb{R}^{p \times m}$ is the feedthrough (or direct transmission) matrix.
\end{itemize}

\subsection{Linear Quadratic Regulator (LQR) Problem}
The LQR problem for continuous-time systems involves finding a control law that minimizes a quadratic cost function. 
The objective is to regulate the state of the system to the origin while minimizing the control effort. The cost function is defined as:

\begin{equation} \label{eq:continuous_cost_function}
    J(t) = \int_{0}^{t} \left( x(\tau)^T Q x(\tau) + u(\tau)^T R u(\tau) \right) d\tau
\end{equation}
where $Q \in \mathbb{R}^{n \times n}$ and $R \in \mathbb{R}^{m \times m}$ determine the relative importance of the state and 
control effort in the cost function.


\subsection{Comparison to Discrete-time LQR}
In discrete-time systems, the state-space representation is defined by difference equations rather than differential equations. 
The discrete-time LQR problem is similar to its continuous-time counterpart but with a cost function summed over discrete time steps. 
The key equations are:

\begin{equation}
    x_{k+1} = A_d x_k + B_d u_k, \quad k = 0, 1, 2, \ldots
\end{equation}
\begin{equation}
    J_N(u^N) = \sum_{k=0}^{N} \left( x_k^T Q x_k + u_k^T R u_k \right) + x_{N+1}^T Q_f x_{N+1}
\end{equation}
\begin{equation}
    u_k = -K_d x_k,
\end{equation}
where $K_d$ is the optimal control gain matrix.


\subsection{The Optimal Control}
In the lecture, we discussed the determination of the control gain matrix \( K \) for discrete-time systems using an iterative approach. 
This method involved solving a finite-horizon cost function through backward iteration, essentially using dynamic programming techniques. 
However, for continuous-time systems, the process leverages the \important{Riccati equation} for an infinite-horizon cost function, 
providing a more direct and analytical solution (\ref{sec:riccati}).





% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\section{Inverted Pendulum on a Cart}

\subsection{Formulation of the Problem}

\subsection{Systen Dynamics}




% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\textbf{\huge{Extra material}}
\section{Riccati Equation}\label{sec:riccati}

This derivation of Riccati equation will provide an example of how to solve convex optimization problems using the \important{calculus of variations}, 
and it will also provide a template for computing the optimal control solution for nonlinear systems. 

First, we will add a terminal cost to our LQR cost function in \ref{eq:continuous_cost_function}, 
and also introduce a factor of 1/2 to simplify computations:
\begin{equation}
    J = \int_{0}^{t_f} \underbrace{\frac{1}{2} \left( x^T Q x + u^T R u \right)}_\text{Lagrangian $\mathcal{L}$} d\tau + 
        \underbrace{\frac{1}{2} x(t_f)^T Q_f x(t_f)}_\text{Terminal cost}
\label{eq:cost_function}
\end{equation}
The goal is to minimize the quadratic cost function \( J \) subject to the dynamical constraint:
\begin{equation}
    \dot{x} = A x + B u.
\label{eq:dynamic_constraint}
\end{equation}
We may solve this using the calculus of variations by introducing the following augmented cost function:
\begin{equation}
    J_{aug} = \int_{0}^{t_f} \left[ \frac{1}{2} \left( x^T Q x + u^T R u \right) + \lambda^T (A x + B u - \dot{x}) \right] 
    dt + \frac{1}{2} x(t_f)^T Q_f x(t_f)
\label{eq:augmented_cost_function}
\end{equation}
The variable \( \lambda \) is a \important{Lagrange multiplier}, called the co-state, that enforces the dynamic constraints. \( \lambda \) may 
take any value and \( J_{aug} = J \) will hold.

\bigbreak

Taking the total variation of \( J_{aug} \) in \ref{eq:augmented_cost_function} yields:
\begin{equation}
    \delta J_{aug} = \int_{0}^{t_f} \left[ \frac{\partial \mathcal{L}}{\partial x} \delta x + \frac{\partial \mathcal{L}}{\partial u} \delta u 
        + \lambda^T A \delta x + \lambda^T B \delta u - \lambda^T \delta \dot{x} \right] dt + Q_f x(t_f) \delta x(t_f)
\label{eq:total_variation}
\end{equation}
The partial derivatives of the Lagrangian are \( \frac{\partial \mathcal{L}}{\partial x} = x^T Q \) and 
\( \frac{\partial \mathcal{L}}{\partial u} = u^T R \). The last term in the integral may be modified using integration by parts:

\begin{equation}
    -\int_{0}^{t_f} \lambda^T \delta \dot{x} dt = -\lambda^T (t_f) \delta x(t_f) + \lambda^T (0) \delta x(0) + 
    \int_{0}^{t_f} \dot{\lambda}^T \delta x dt.
\label{eq:integration_by_parts}
\end{equation}
The term \( \lambda^T (0) \delta x(0) \) is equal to zero, or else the control system would be non-causal 
(i.e., then future control could change the initial condition of the system).

\bigbreak

Finally, the total variation of the augmented cost function in \ref{eq:total_variation} simplifies as follows:
\begin{equation}
    \delta J_{aug} = \int_{0}^{t_f} \left[ x^T Q + \lambda^T A + \dot{\lambda}^T \right] \delta x dt + 
    \int_{0}^{t_f} \left[ u^T R + \lambda^T B \right] \delta u dt + (x(t_f)^T Q_f - \lambda^T (t_f)) \delta x(t_f).
\label{eq:simplified_total_variation}
\end{equation}
\textbf{Each variation term in \ref{eq:simplified_total_variation} must equal zero for an optimal control solution that minimizes \( J \).} 
Thus, we may break this up into three equations:
\begin{equation}
    x^T Q + \lambda^T A + \dot{\lambda}^T = 0
\label{eq:variation_term_1}
\end{equation}
\begin{equation}
    u^T R + \lambda^T B = 0
\label{eq:variation_term_2}
\end{equation}
\begin{equation}
    x(t_f)^T Q_f - \lambda^T (t_f) = 0. 
\label{eq:variation_term_3}
\end{equation}
Note that the constraint in \ref{eq:variation_term_3} represents an initial condition for the reverse-time equation for 
\( \lambda \) starting at \( t_f \). Thus, the dynamics in \ref{eq:dynamic_constraint} with initial condition \( x(0) = x_0 \) 
and \ref{eq:variation_term_3} with the final-time condition \( \lambda (t_f) = Q_f x(t_f) \) form a two-point boundary value problem. 
This may be integrated numerically to find the optimal control solution, even for nonlinear systems.

\bigbreak

Because the dynamics are linear, it is possible to posit the form \( \lambda = P x \), and substitute into \ref{eq:variation_term_1} above. 
The first equation becomes:
\begin{equation}
    \left( \dot{P} x + P \dot{x} \right)^T + x^T Q + \lambda^T A = 0.
\label{eq:posit_form}
\end{equation}
Taking the transpose, and substituting \ref{eq:dynamic_constraint} in for \( \dot{x} \), yields:
\begin{equation}
    \dot{P} x + P (A x + B u) + Q x + A^T P x = 0.
\label{eq:transpose_substitution}
\end{equation}
From \ref{eq:variation_term_2}, we have
\begin{equation}
    u = -R^{-1} B^T \lambda = -R^{-1} B^T P x.
\label{eq:control_input}
\end{equation}
Finally, combining yields:
\begin{equation}
    \dot{P} x + P A x + A^T P x - P B R^{-1} B^T P x + Q x = 0.
\label{eq:combining_yields}
\end{equation}
This equation must be true for all \( x \), and so it may also be written as a matrix equation.
 Dropping the terminal cost and letting time go to infinity, the \( \dot{P} \) term disappears, and we recover the
\important{Continuous Algebraic Riccati equation (CARE) }:
\begin{equation}
    P A + A^T P - P B R^{-1} B^T P + Q = 0.
\label{eq:riccati_equation}
\end{equation}
Although this procedure is somewhat involved, each step is relatively straightforward. In addition, the dynamics in \ref{eq:dynamic_constraint} 
may be replaced with nonlinear dynamics \( \dot{x} = f(x,u) \), and a similar nonlinear two-point boundary value problem may be formulated 
with \( \frac{\partial f}{\partial x} \) replacing \( A \) and \( \frac{\partial f}{\partial u} \) replacing \( B \). 
This procedure is extremely general, and may be used to numerically obtain nonlinear optimal control trajectories.




% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{Euler-Lagrange Equations}




% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\begin{thebibliography}{9}

    \bibitem{brunton2019}
    S. Brunton, and J. Kutz,
    \textit{Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control}.
    Cambridge: Cambridge University Press, 2019.
    doi:10.1017/9781108380690
    
    \bibitem{brunton2017}
    S. Brunton,
    \textit{Linear Quadratic Regulator (LQR) Control for the Inverted Pendulum on a Cart [Control Bootcamp]}, 2017.
    
\end{thebibliography}

\end{document} 
